
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#from functools32 import lru_cache\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "def tokens(sentence):\n",
    "  return word_tokenize(sentence)\n",
    "\n",
    "def lemmatize(tokens):\n",
    "  wnl = WordNetLemmatizer()\n",
    "  return [wnl.lemmatize(token) for token in tokens]\n",
    "\n",
    "def postags(tokens):\n",
    "  return nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Daniel']\n",
      "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Daniel', 'NNP')]\n",
      "../STS-data/STS2012-gold/STS.input.MSRpar.txt\n",
      "../STS-data/STS2012-gold/STS.input.MSRvid.txt\n",
      "../STS-data/STS2012-gold/STS.input.SMTeuroparl.txt\n",
      "../STS-data/STS2012-gold/STS.input.surprise.OnWN.txt\n",
      "../STS-data/STS2012-gold/STS.input.surprise.SMTnews.txt\n",
      "../STS-data/STS2012-train/STS.input.MSRpar.txt\n",
      "../STS-data/STS2012-train/STS.input.MSRvid.txt\n",
      "../STS-data/STS2012-train/STS.input.SMTeuroparl.txt\n",
      "../STS-data/STS2013-gold/STS.input.FNWN.txt\n",
      "../STS-data/STS2013-gold/STS.input.headlines.txt\n",
      "../STS-data/STS2013-gold/STS.input.OnWN.txt\n",
      "../STS-data/STS2014-gold/STS.input.deft-forum.txt\n",
      "../STS-data/STS2014-gold/STS.input.deft-news.txt\n",
      "../STS-data/STS2014-gold/STS.input.headlines.txt\n",
      "../STS-data/STS2014-gold/STS.input.images.txt\n",
      "../STS-data/STS2014-gold/STS.input.OnWN.txt\n",
      "../STS-data/STS2014-gold/STS.input.tweet-news.txt\n",
      "../STS-data/STS2015-gold/STS.input.answers-forums.txt\n",
      "../STS-data/STS2015-gold/STS.input.answers-students.txt\n",
      "../STS-data/STS2015-gold/STS.input.belief.txt\n",
      "../STS-data/STS2015-gold/STS.input.headlines.txt\n",
      "../STS-data/STS2015-gold/STS.input.images.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.input.answer-answer.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.gs.answer-answer.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.input.headlines.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.gs.headlines.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.input.plagiarism.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.gs.plagiarism.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.input.postediting.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.gs.postediting.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.input.question-question.txt\n",
      "../STS-data/sts2016-english-with-gs-v1.0/STS2016.gs.question-question.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Is it inappropriate to spend personal resources for the company's benefit?\",\n",
       " \"Is it ethical/legal to use the University's resources for a personal project?\",\n",
       " '\\n']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"My name is Daniel\"\n",
    "word_tokens = tokens(sentence)\n",
    "print lemmatize(word_tokens)\n",
    "print postags(word_tokens)\n",
    "\n",
    "import glob\n",
    "import re\n",
    "\n",
    "fname = \"../STS-data/STS2013-gold/STS.input.headlines.txt\"\n",
    "with open(fname) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "Data = []\n",
    "    \n",
    "for fname in glob.glob(\"../STS-data/*201[1-5]*/*input*.txt\"):\n",
    "    print fname\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if len(line.split('\\t')) !=2 :\n",
    "                print fname\n",
    "                break;\n",
    "            else:\n",
    "                Data.append(line.split('\\t'))\n",
    "for fname in glob.glob(\"../STS-data/*2016*/*input*.txt\"):\n",
    "    print fname\n",
    "    print re.sub(\"input\",\"gs\",fname)\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "        with open(re.sub(\"input\",\"gs\",fname)) as nfile:\n",
    "            nlines = nfile.readlines()\n",
    "        #nlines[index]\n",
    "        for index,line in enumerate(lines):\n",
    "            if len(line.split('\\t')) !=4 :\n",
    "                #print fname\n",
    "                break;\n",
    "            else:\n",
    "                x = line.split('\\t')\n",
    "                Data.append([x[0],x[1],nlines[index]])\n",
    "Data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data = []\n",
    "fname = \"../STS-data/STS2013-gold/STS.input.headlines.txt\"\n",
    "with open(fname) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    with open(re.sub(\"input\",\"gs\",fname)) as nfile:\n",
    "        nlines = nfile.readlines()\n",
    "        for index,line in enumerate(lines):\n",
    "            if len(line.split('\\t')) !=2 :\n",
    "                #print fname\n",
    "                break;\n",
    "            else:\n",
    "                if(re.search(\"(\\d)((\\.)(\\d))?\", nlines[index])):\n",
    "                    ;#print nlines[index]\n",
    "                else:\n",
    "                    print lines[index]\n",
    "                    break\n",
    "                x = line.split('\\t')\n",
    "                Data.append([x[0],x[1], float(re.sub(\"\\n\",\"\",nlines[index]))])\n",
    "                \n",
    "#print Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data = []\n",
    "fname = \"../STS-data/sts2016-english-with-gs-v1.0/STS2016.input.question-question.txt\"\n",
    "with open(fname) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    with open(re.sub(\"input\",\"gs\",fname)) as nfile:\n",
    "        nlines = nfile.readlines()\n",
    "        #print nlines\n",
    "        \n",
    "        for index,line in enumerate(lines):\n",
    "            if len(line.split('\\t')) !=4 :\n",
    "                #print fname\n",
    "                break;\n",
    "            else:\n",
    "                if(not re.search(\"(\\d)((\\.)(\\d))?\", nlines[index])):\n",
    "                    continue;#print nlines[index]\n",
    "                #else:\n",
    "                    #print lines[index]\n",
    "                    #continue\n",
    "                x = line.split('\\t')\n",
    "                Data.append([x[0],x[1], float(re.sub(\"\\n\",\"\",nlines[index]))])\n",
    "                \n",
    "#print Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import preprocess as process\n",
    "import utilities as utility\n",
    "import math\n",
    "\n",
    "def ngram_vector_keys(tokens, ngram_size=1):\n",
    "  vector_keys = []\n",
    "  for i in range(len(tokens)-(ngram_size-1)):\n",
    "    vector_keys.append(tuple(tokens[i:i+(ngram_size)]))\n",
    "    # vector_keys.append(\" \".join(tokens[i:i+(ngram_size)]))\n",
    "  return vector_keys\n",
    "\n",
    "def containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size=1):\n",
    "  set1 = set(ngram_vector_keys(sent_1_tokens, ngram_size))\n",
    "  set2 = set(ngram_vector_keys(sent_2_tokens, ngram_size))\n",
    "  containment_of_sentence_1_in_2 = float(len(set1.intersection(set2)))/len(set1)\n",
    "  containment_of_sentence_2_in_1 = float(len(set1.intersection(set2)))/len(set2)\n",
    "  return containment_of_sentence_1_in_2, containment_of_sentence_2_in_1\n",
    "\n",
    "def JaccardCoefficient(sent_1_tokens, sent_2_tokens, ngram_size=1):\n",
    "  set1 = set(ngram_vector_keys(sent_1_tokens, ngram_size))\n",
    "  set2 = set(ngram_vector_keys(sent_2_tokens, ngram_size))\n",
    "  #print set1,set2\n",
    "  return float(len(set1.intersection(set2)))/len(set1.union(set2))\n",
    "\n",
    "def TFIDF(documents):\n",
    "  Vocabulary = Counter()\n",
    "  DocVectors = []\n",
    "  IDFVector = Counter()\n",
    "  No_of_Documents = float(len(documents))\n",
    "  for document in documents:\n",
    "    tf_single_doc_count = Counter(process.tokens(document))\n",
    "    Vocabulary+= tf_single_doc_count\n",
    "    DocVectors.append(tf_single_doc_count)\n",
    "    IDFVector += Counter(tf_single_doc_count.keys())\n",
    "  # print IDFVector\n",
    "  for key in IDFVector.keys():\n",
    "    IDFVector[key] = math.log(No_of_Documents/(1+IDFVector[key]))\n",
    "  # print IDFVector\n",
    "  # IDFVector = defaultdict(lambda:0.0, dict((key,Vocabulary[key]*) for key in c.keys()))\n",
    "  TFIDFScores = defaultdict(lambda:0.0, dict((key,Vocabulary[key]*IDFVector[key]) for key in Vocabulary.keys()))\n",
    "  # print TFIDFScores, IDFVector, Vocabulary\n",
    "  return TFIDFScores, Vocabulary, DocVectors, IDFVector\n",
    "\n",
    "def DocvectorTFIDF(TFIDFScores, tokens):\n",
    "  return defaultdict(lambda:0.0, dict((key,TFIDFScores[key] if key in TFIDFScores.keys() else 0.0) for key in tokens))\n",
    "\n",
    "\n",
    "def cosinesimilarity(document1, document2, TFIDFScores):\n",
    "  tokens1 = set(process.tokens(document1))\n",
    "  tokens2 = set(process.tokens(document2))\n",
    "  vector1 = DocvectorTFIDF(TFIDFScores, tokens1)\n",
    "  vector2 = DocvectorTFIDF(TFIDFScores, tokens2)\n",
    "  len_vector_1 = math.sqrt(sum({k: v**2 for k, v in vector1.items()}.values()))\n",
    "  print len_vector_1\n",
    "  len_vector_2 = math.sqrt(sum({k: v**2 for k, v in vector1.items()}.values()))\n",
    "  print len_vector_2\n",
    "  print utility.dict_dotprod(vector1,vector2)\n",
    "  cosine_similarity_score = (utility.dict_dotprod(vector1,vector2))/float((len_vector_2*len_vector_1))\n",
    "  return cosine_similarity_score\n",
    "\n",
    "\n",
    "def cosinesimilarity_without_TFIDF(document1, document2):\n",
    "  vector1 = Counter(process.tokens(document1))\n",
    "  vector2 = Counter(process.tokens(document2))\n",
    "  print vector1\n",
    "  print vector2\n",
    "  len_vector_1 = math.sqrt(sum({k: v**2 for k, v in vector1.items()}.values()))\n",
    "  print len_vector_1\n",
    "  len_vector_2 = math.sqrt(sum({k: v**2 for k, v in vector1.items()}.values()))\n",
    "  print len_vector_2\n",
    "  cosine_similarity_score = (utility.dict_dotprod(vector1,vector2))/float((len_vector_2*len_vector_1))\n",
    "  return cosine_similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.41421356237\n",
      "1.41421356237\n",
      "1.0\n",
      "0.5\n",
      "Counter({'i': 1, 'ike': 1})\n",
      "Counter({'i': 1, 'ike': 1})\n",
      "1.41421356237\n",
      "1.41421356237\n",
      "1.0\n",
      "(1.0, 0.6)\n",
      "0.6\n",
      "0.5\n",
      "0.428571428571\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "TFIDFScores, Vocabulary, DocVectors, IDFVector = TFIDF([\"i ike\", \"i ike\"])\n",
    "TFIDFScores = {\"i\":1,\"ike\":1}\n",
    "v = DocvectorTFIDF(TFIDFScores, process.tokens(\"i ike\"))\n",
    "#print v\n",
    "#print utility.dict_dotprod(v,DocvectorTFIDF(TFIDFScores, process.tokens(\"i tes\")))\n",
    "print cosinesimilarity(\"i ike\", \"i tes\", {\"i\":1,\"ike\":1})\n",
    "print cosinesimilarity_without_TFIDF(\"i ike\", \"i ike\")\n",
    "\n",
    "JaccardCoefficient([\"sent_1_tokens\",\"test\"], [\"sent_2_tokens\",\"tes\",\"Tesd\",\"Tesd\",\"Tesd\"], ngram_size=2)\n",
    "print containment_coefficienct([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=1)\n",
    "print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=1)\n",
    "print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=2)\n",
    "print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=3)\n",
    "print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "def ngram_vector_keys(tokens, ngram_size=1):\n",
    "  vector_keys = []\n",
    "  for i in range(len(tokens)-(ngram_size-1)):\n",
    "    vector_keys.append(tuple(tokens[i:i+(ngram_size)]))\n",
    "    # vector_keys.append(\" \".join(tokens[i:i+(ngram_size)]))\n",
    "  return vector_keys\n",
    "\n",
    "def character_ngram_vector_keys(tokens, ngram_size=2):\n",
    "  vector_keys = []\n",
    "  for token in tokens:\n",
    "    for i in range(len(token)-(ngram_size-1)):\n",
    "      vector_keys.append(tuple(token[i:i+(ngram_size)]))\n",
    "  return vector_keys\n",
    "\n",
    "def ngram_keys(sent_1_tokens, sent_2_tokens, ngram_size, character_ngram):\n",
    "  if character_ngram:\n",
    "    set1 = set(character_ngram_vector_keys(sent_1_tokens, ngram_size))\n",
    "    set2 = set(character_ngram_vector_keys(sent_2_tokens, ngram_size))\n",
    "  else:\n",
    "    set1 = set(ngram_vector_keys(sent_1_tokens, ngram_size))\n",
    "    set2 = set(ngram_vector_keys(sent_2_tokens, ngram_size))\n",
    "  print set1,set2\n",
    "  return set1, set2\n",
    "\n",
    "def containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size=1, character_ngram=False):\n",
    "  set1, set2 = ngram_keys(sent_1_tokens, sent_2_tokens, ngram_size, character_ngram)\n",
    "  intersecting_keys_len = len(set1.intersection(set2))\n",
    "  containment_of_sentence_1_in_2 = float(intersecting_keys_len)/len(set1)\n",
    "  containment_of_sentence_2_in_1 = float(intersecting_keys_len)/len(set2)\n",
    "  # return containment_of_sentence_1_in_2, containment_of_sentence_2_in_1\n",
    "  return numpy.mean([containment_of_sentence_1_in_2, containment_of_sentence_2_in_1])\n",
    "\n",
    "def JaccardCoefficient(sent_1_tokens, sent_2_tokens, ngram_size=1, character_ngram=False):\n",
    "  set1, set2 = ngram_keys(sent_1_tokens, sent_2_tokens, ngram_size, character_ngram)\n",
    "  return float(len(set1.intersection(set2)))/len(set1.union(set2))\n",
    "\n",
    "def POSTags_JaccardCoefficient_and_containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size=1):\n",
    "  postag_tokens_1 = process.postags(sent_1_tokens)\n",
    "  postag_tokens_2 = process.postags(sent_2_tokens)\n",
    "  return JaccardCoefficient(postag_tokens_1, postag_tokens_2, ngram_size), containment_coefficienct(postag_tokens_1, postag_tokens_2, ngram_size)\n",
    "\n",
    "def Lemma_JaccardCoefficient_and_containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size=1):\n",
    "  lemma_tokens_1 = process.lemmatize(sent_1_tokens)\n",
    "  lemma_tokens_2 = process.lemmatize(sent_2_tokens)\n",
    "  return JaccardCoefficient(lemma_tokens_1, lemma_tokens_2, ngram_size), containment_coefficienct(lemma_tokens_1, lemma_tokens_2, ngram_size)\n",
    "\n",
    "def character_ngram_JaccardCoefficient_and_containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size=1, character_ngram=True):\n",
    "  return JaccardCoefficient(sent_1_tokens, sent_2_tokens, ngram_size, character_ngram), containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size, character_ngram)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([('is',), ('a',), ('rose',)]) set([('is',), ('a',), ('flower',), ('which',), ('rose',)])\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "#print ngram_keys([\"sent_1_tokens\",\"test\"], [\"sent_2_tokens\",\"tes\",\"Tesd\",\"Tesd\",\"Tesd\"], ngram_size=2,character_ngram=False)\n",
    "print containment_coefficienct([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=1)\n",
    "#print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=1)\n",
    "#print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=2)\n",
    "#print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=3)\n",
    "#print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([('o', 's'), ('s', 'e'), ('r', 'o'), ('i', 's')]) set([('f', 'l'), ('i', 'c'), ('e', 'r'), ('l', 'o'), ('h', 'i'), ('o', 's'), ('s', 'e'), ('r', 'o'), ('w', 'e'), ('w', 'h'), ('c', 'h'), ('i', 's'), ('o', 'w')])\n",
      "set([('o', 's'), ('s', 'e'), ('r', 'o'), ('i', 's')]) set([('f', 'l'), ('i', 'c'), ('e', 'r'), ('l', 'o'), ('h', 'i'), ('o', 's'), ('s', 'e'), ('r', 'o'), ('w', 'e'), ('w', 'h'), ('c', 'h'), ('i', 's'), ('o', 'w')])\n",
      "set([(('rose', 'VBD'), ('is', 'VBZ')), (('a', 'DT'), ('rose', 'VBD')), (('is', 'VBZ'), ('a', 'DT'))]) set([(('a', 'DT'), ('rose', 'VBD')), (('is', 'VBZ'), ('a', 'DT')), (('rose', 'VBD'), ('is', 'VBZ')), (('which', 'WDT'), ('is', 'VBZ')), (('a', 'DT'), ('flower', 'NN')), (('flower', 'NN'), ('which', 'WDT'))])\n",
      "set([(('rose', 'VBD'), ('is', 'VBZ')), (('a', 'DT'), ('rose', 'VBD')), (('is', 'VBZ'), ('a', 'DT'))]) set([(('a', 'DT'), ('rose', 'VBD')), (('is', 'VBZ'), ('a', 'DT')), (('rose', 'VBD'), ('is', 'VBZ')), (('which', 'WDT'), ('is', 'VBZ')), (('a', 'DT'), ('flower', 'NN')), (('flower', 'NN'), ('which', 'WDT'))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0.75)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_1_tokens, sent_2_tokens = [\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"]\n",
    "character_ngram_JaccardCoefficient_and_containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size=2, character_ngram=True)\n",
    "POSTags_JaccardCoefficient_and_containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x116cffb90>, {'i': 0.0, 'test': 0.6931471805599453, 'hi': 0.6931471805599453, 'ike': 0.0})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({1: 1, 2: 1})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDFScores, Vocabulary, DocVectors, IDFVector = TFIDF([\"i ike\", \"i ike\", \"i test\", \"hi ike\"])\n",
    "print TFIDFScores\n",
    "x = [(1,2,3,4),(1,2,13,4)]\n",
    "[ (t[2]) for t in x]\n",
    "Counter(set([1,1,2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CharacterIDFVector(documents, ngram_size=2):\n",
    "  No_of_Documents = float(len(documents))\n",
    "  IDFVector = Counter()\n",
    "  for document in documents:\n",
    "    tokens = process.tokens(document)\n",
    "    IDFVector += Counter(set(character_ngram_vector_keys(tokens, ngram_size)))\n",
    "  #print IDFVector\n",
    "  for key in IDFVector.keys():\n",
    "    IDFVector[key] = math.log(No_of_Documents/(1+IDFVector[key]))\n",
    "  return IDFVector\n",
    "\n",
    "def ngram_vector_keys(tokens, ngram_size=1):\n",
    "  vector_keys = []\n",
    "  for i in range(len(tokens)-(ngram_size-1)):\n",
    "    vector_keys.append(tuple(tokens[i:i+(ngram_size)]))\n",
    "    # vector_keys.append(\" \".join(tokens[i:i+(ngram_size)]))\n",
    "  return vector_keys\n",
    "\n",
    "def character_ngram_vector_keys(tokens, ngram_size=2):\n",
    "  vector_keys = []\n",
    "  for token in tokens:\n",
    "    for i in range(len(token)-(ngram_size-1)):\n",
    "      vector_keys.append(tuple(token[i:i+(ngram_size)]))\n",
    "  return vector_keys\n",
    "\n",
    "def ngram_keys(sent_1_tokens, sent_2_tokens, ngram_size, character_ngram):\n",
    "  if character_ngram:\n",
    "    set1 = set(character_ngram_vector_keys(sent_1_tokens, ngram_size))\n",
    "    set2 = set(character_ngram_vector_keys(sent_2_tokens, ngram_size))\n",
    "  else:\n",
    "    set1 = set(ngram_vector_keys(sent_1_tokens, ngram_size))\n",
    "    set2 = set(ngram_vector_keys(sent_2_tokens, ngram_size))\n",
    "  return set1, set2\n",
    "\n",
    "def ngram_weighted_value(keys, IDFScores, character_ngram=False):\n",
    "  value = 0.0\n",
    "  if not character_ngram:\n",
    "    for key in keys:\n",
    "      for i in range(len(key)):# This is a tuple\n",
    "        if key[i] in IDFScores.keys():\n",
    "          value += IDFScores[key[i]]\n",
    "  else:\n",
    "    for key in keys:\n",
    "      if key in IDFScores.keys():\n",
    "        value+=IDFScores[key]\n",
    "  return value\n",
    "\n",
    "\n",
    "def similarity_score(set1, set2, IDFScores=None, character_ngram=False, ngram_weighing=False):\n",
    "  if not ngram_weighing:\n",
    "    numerator = len(set1)\n",
    "    denominator = len(set2)\n",
    "  else:\n",
    "    print set1\n",
    "    print set2\n",
    "    print character_ngram\n",
    "    print IDFScores\n",
    "    numerator = ngram_weighted_value(set1, IDFScores, character_ngram)\n",
    "    denominator = ngram_weighted_value(set2, IDFScores, character_ngram)\n",
    "    # INCASE IDF Values are not present for it\n",
    "    if denominator == 0:\n",
    "      numerator = len(set1)\n",
    "      denominator = len(set2)\n",
    "  return numerator/float(denominator)\n",
    "\n",
    "def containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size=1, character_ngram=False, ngram_weighing=False, IDFScores=None):\n",
    "  set1, set2 = ngram_keys(sent_1_tokens, sent_2_tokens, ngram_size, character_ngram)\n",
    "  intersecting_keys = (set1.intersection(set2))\n",
    "  containment_of_sentence_1_in_2 = similarity_score(intersecting_keys, set1, IDFScores, character_ngram, ngram_weighing)\n",
    "  containment_of_sentence_2_in_1 = similarity_score(intersecting_keys, set2, IDFScores, character_ngram, ngram_weighing)\n",
    "  # return containment_of_sentence_1_in_2, containment_of_sentence_2_in_1\n",
    "  return numpy.mean([containment_of_sentence_1_in_2, containment_of_sentence_2_in_1])\n",
    "\n",
    "def JaccardCoefficient(sent_1_tokens, sent_2_tokens, ngram_size=1, character_ngram=False, ngram_weighing=False, IDFScores=None):\n",
    "  set1, set2 = ngram_keys(sent_1_tokens, sent_2_tokens, ngram_size, character_ngram)\n",
    "  \n",
    "  return similarity_score(set1.intersection(set2), set1.union(set2), IDFScores, character_ngram, ngram_weighing)\n",
    "  # return float(len(set1.intersection(set2)))/len(set1.union(set2))\n",
    "\n",
    "def POSTags_JaccardCoefficient_and_containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size=1, ngram_weighing=False, IDFScores=None):\n",
    "  postag_tokens_1 = process.postags(sent_1_tokens)\n",
    "  postag_tokens_2 = process.postags(sent_2_tokens)\n",
    "  return JaccardCoefficient(postag_tokens_1, postag_tokens_2, ngram_size, False, ngram_weighing, IDFScores), containment_coefficienct(postag_tokens_1, postag_tokens_2, ngram_size, False, ngram_weighing, IDFScores)\n",
    "\n",
    "def Lemma_JaccardCoefficient_and_containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size=1, ngram_weighing=False, IDFScores=None):\n",
    "  lemma_tokens_1 = process.lemmatize(sent_1_tokens)\n",
    "  lemma_tokens_2 = process.lemmatize(sent_2_tokens)\n",
    "  return JaccardCoefficient(lemma_tokens_1, lemma_tokens_2, ngram_size, False, ngram_weighing, IDFScores), containment_coefficienct(lemma_tokens_1, lemma_tokens_2, ngram_size, False, ngram_weighing, IDFScores)\n",
    "\n",
    "def character_ngram_JaccardCoefficient_and_containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size=2, character_ngram=True, ngram_weighing=False, IDFScores=None):\n",
    "  return JaccardCoefficient(sent_1_tokens, sent_2_tokens, ngram_size, character_ngram, ngram_weighing, IDFScores), containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size, character_ngram, ngram_weighing, IDFScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('t', 'o'): 0.4054651081081644, ('k', 'e'): 0.4054651081081644, ('e', 's'): 0.4054651081081644, ('h', 'i'): 0.4054651081081644, ('i', 'k'): 0.4054651081081644, ('o', 'm'): 0.4054651081081644, ('t', 'e'): 0.4054651081081644, ('s', 't'): 0.4054651081081644})\n",
      "Counter({'test': 0.6931471805599453, 'hi': 0.6931471805599453, 'i': 0.0, 'ike': 0.0})\n",
      "set([('i',)])\n",
      "set([('test',), ('i',), ('rest',)])\n",
      "False\n",
      "Counter({'test': 0.6931471805599453, 'hi': 0.6931471805599453, 'i': 0.0, 'ike': 0.0})\n",
      "set([('i',)])\n",
      "set([('test',), ('i',)])\n",
      "False\n",
      "Counter({'test': 0.6931471805599453, 'hi': 0.6931471805599453, 'i': 0.0, 'ike': 0.0})\n",
      "set([('i',)])\n",
      "set([('i',), ('rest',)])\n",
      "False\n",
      "Counter({'test': 0.6931471805599453, 'hi': 0.6931471805599453, 'i': 0.0, 'ike': 0.0})\n",
      "(0.0, 0.25)\n",
      "set([('o', 's'), ('s', 'e'), ('r', 'o'), ('i', 's')])\n",
      "set([('f', 'l'), ('i', 'c'), ('h', 'i'), ('s', 'e'), ('w', 'e'), ('w', 'h'), ('o', 'w'), ('c', 'h'), ('e', 'r'), ('l', 'o'), ('o', 's'), ('r', 'o'), ('i', 's')])\n",
      "True\n",
      "Counter({('t', 'o'): 0.4054651081081644, ('k', 'e'): 0.4054651081081644, ('e', 's'): 0.4054651081081644, ('h', 'i'): 0.4054651081081644, ('i', 'k'): 0.4054651081081644, ('o', 'm'): 0.4054651081081644, ('t', 'e'): 0.4054651081081644, ('s', 't'): 0.4054651081081644})\n",
      "set([('o', 's'), ('s', 'e'), ('r', 'o'), ('i', 's')])\n",
      "set([('o', 's'), ('s', 'e'), ('r', 'o'), ('i', 's')])\n",
      "True\n",
      "Counter({('t', 'o'): 0.4054651081081644, ('k', 'e'): 0.4054651081081644, ('e', 's'): 0.4054651081081644, ('h', 'i'): 0.4054651081081644, ('i', 'k'): 0.4054651081081644, ('o', 'm'): 0.4054651081081644, ('t', 'e'): 0.4054651081081644, ('s', 't'): 0.4054651081081644})\n",
      "set([('o', 's'), ('s', 'e'), ('r', 'o'), ('i', 's')])\n",
      "set([('f', 'l'), ('i', 'c'), ('e', 'r'), ('l', 'o'), ('h', 'i'), ('o', 's'), ('s', 'e'), ('r', 'o'), ('w', 'e'), ('w', 'h'), ('c', 'h'), ('i', 's'), ('o', 'w')])\n",
      "True\n",
      "Counter({('t', 'o'): 0.4054651081081644, ('k', 'e'): 0.4054651081081644, ('e', 's'): 0.4054651081081644, ('h', 'i'): 0.4054651081081644, ('i', 'k'): 0.4054651081081644, ('o', 'm'): 0.4054651081081644, ('t', 'e'): 0.4054651081081644, ('s', 't'): 0.4054651081081644})\n",
      "(0.0, 0.5)\n",
      "set([('e', 's'), ('s', 't')])\n",
      "set([('t', 'e'), ('e', 's'), ('s', 't'), ('r', 'e')])\n",
      "True\n",
      "Counter({('t', 'o'): 0.4054651081081644, ('k', 'e'): 0.4054651081081644, ('e', 's'): 0.4054651081081644, ('h', 'i'): 0.4054651081081644, ('i', 'k'): 0.4054651081081644, ('o', 'm'): 0.4054651081081644, ('t', 'e'): 0.4054651081081644, ('s', 't'): 0.4054651081081644})\n",
      "set([('e', 's'), ('s', 't')])\n",
      "set([('e', 's'), ('t', 'e'), ('s', 't')])\n",
      "True\n",
      "Counter({('t', 'o'): 0.4054651081081644, ('k', 'e'): 0.4054651081081644, ('e', 's'): 0.4054651081081644, ('h', 'i'): 0.4054651081081644, ('i', 'k'): 0.4054651081081644, ('o', 'm'): 0.4054651081081644, ('t', 'e'): 0.4054651081081644, ('s', 't'): 0.4054651081081644})\n",
      "set([('e', 's'), ('s', 't')])\n",
      "set([('e', 's'), ('s', 't'), ('r', 'e')])\n",
      "True\n",
      "Counter({('t', 'o'): 0.4054651081081644, ('k', 'e'): 0.4054651081081644, ('e', 's'): 0.4054651081081644, ('h', 'i'): 0.4054651081081644, ('i', 'k'): 0.4054651081081644, ('o', 'm'): 0.4054651081081644, ('t', 'e'): 0.4054651081081644, ('s', 't'): 0.4054651081081644})\n",
      "(0.6666666666666666, 0.83333333333333326)\n"
     ]
    }
   ],
   "source": [
    "IDFScores = CharacterIDFVector([\"i ike\", \"hi tom\",\"test\"],2)\n",
    "print IDFScores\n",
    "TFIDFScores, Vocabulary, DocVectors, IDFVector = TFIDF([\"i ike\", \"i ike\", \"i test\", \"hi ike\"])\n",
    "print IDFVector\n",
    "tok1=[\"i\",\"test\"]\n",
    "tok2=[\"i\",\"rest\"]\n",
    "print Lemma_JaccardCoefficient_and_containment_coefficienct(tok1, tok2, ngram_size=1, ngram_weighing=True, IDFScores=IDFVector)\n",
    "print character_ngram_JaccardCoefficient_and_containment_coefficienct(sent_1_tokens, sent_2_tokens, 2, True, True, IDFScores)\n",
    "print character_ngram_JaccardCoefficient_and_containment_coefficienct(tok1, tok2, 2, True, True, IDFScores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.6\n",
      "0.5\n",
      "0.428571428571\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "print containment_coefficienct([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=1)\n",
    "print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=1)\n",
    "print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=2)\n",
    "print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=3)\n",
    "print JaccardCoefficient([\"a\",\"rose\",\"is\",\"a\",\"rose\",\"is\",\"a\",\"rose\"],[\"a\",\"rose\",\"is\",\"a\",\"flower\",\"which\",\"is\",\"a\",\"rose\"],ngram_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([(('hi', 'JJ'), ('test', 'NN'))])\n",
      "set([(('test', 'NN'), ('this', 'DT')), (('hi', 'JJ'), ('test', 'NN'))])\n",
      "False\n",
      "Counter({'test': 0.6931471805599453, 'hi': 0.6931471805599453, 'i': 0.0, 'ike': 0.0})\n",
      "set([(('hi', 'JJ'), ('test', 'NN'))])\n",
      "set([(('hi', 'JJ'), ('test', 'NN'))])\n",
      "False\n",
      "Counter({'test': 0.6931471805599453, 'hi': 0.6931471805599453, 'i': 0.0, 'ike': 0.0})\n",
      "set([(('hi', 'JJ'), ('test', 'NN'))])\n",
      "set([(('test', 'NN'), ('this', 'DT')), (('hi', 'JJ'), ('test', 'NN'))])\n",
      "False\n",
      "Counter({'test': 0.6931471805599453, 'hi': 0.6931471805599453, 'i': 0.0, 'ike': 0.0})\n",
      "(0.5, 0.75)\n"
     ]
    }
   ],
   "source": [
    "tok1=[\"hi\",\"test\"]\n",
    "tok2=[\"hi\",\"test\", \"this\"]\n",
    "print POSTags_JaccardCoefficient_and_containment_coefficienct(tok1, tok2, 2, True, IDFVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([('test',), ('hi',)])\n",
      "set([('this',), ('test',), ('hi',)])\n",
      "False\n",
      "Counter({'test': 0.6931471805599453, 'hi': 0.6931471805599453, 'i': 0.0, 'ike': 0.0})\n",
      "set([('test',), ('hi',)])\n",
      "set([('test',), ('hi',)])\n",
      "False\n",
      "Counter({'test': 0.6931471805599453, 'hi': 0.6931471805599453, 'i': 0.0, 'ike': 0.0})\n",
      "set([('test',), ('hi',)])\n",
      "set([('this',), ('test',), ('hi',)])\n",
      "False\n",
      "Counter({'test': 0.6931471805599453, 'hi': 0.6931471805599453, 'i': 0.0, 'ike': 0.0})\n",
      "(1.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "print Lemma_JaccardCoefficient_and_containment_coefficienct(tok1, tok2, ngram_size=1, ngram_weighing=True, IDFScores=IDFVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([('e', 's'), ('h', 'i'), ('t', 'e'), ('s', 't')])\n",
      "set([('e', 's'), ('h', 'i'), ('t', 'e'), ('s', 't')])\n",
      "True\n",
      "Counter({('t', 'o'): 0.4054651081081644, ('k', 'e'): 0.4054651081081644, ('e', 's'): 0.4054651081081644, ('h', 'i'): 0.4054651081081644, ('i', 'k'): 0.4054651081081644, ('o', 'm'): 0.4054651081081644, ('t', 'e'): 0.4054651081081644, ('s', 't'): 0.4054651081081644})\n",
      "set([('e', 's'), ('h', 'i'), ('t', 'e'), ('s', 't')])\n",
      "set([('e', 's'), ('h', 'i'), ('t', 'e'), ('s', 't')])\n",
      "True\n",
      "Counter({('t', 'o'): 0.4054651081081644, ('k', 'e'): 0.4054651081081644, ('e', 's'): 0.4054651081081644, ('h', 'i'): 0.4054651081081644, ('i', 'k'): 0.4054651081081644, ('o', 'm'): 0.4054651081081644, ('t', 'e'): 0.4054651081081644, ('s', 't'): 0.4054651081081644})\n",
      "set([('e', 's'), ('h', 'i'), ('t', 'e'), ('s', 't')])\n",
      "set([('e', 's'), ('h', 'i'), ('t', 'e'), ('s', 't')])\n",
      "True\n",
      "Counter({('t', 'o'): 0.4054651081081644, ('k', 'e'): 0.4054651081081644, ('e', 's'): 0.4054651081081644, ('h', 'i'): 0.4054651081081644, ('i', 'k'): 0.4054651081081644, ('o', 'm'): 0.4054651081081644, ('t', 'e'): 0.4054651081081644, ('s', 't'): 0.4054651081081644})\n",
      "(1.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "print character_ngram_JaccardCoefficient_and_containment_coefficienct(tok1, tok1, 2, True, True, IDFScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "  Data = data_collection.Data();\n",
    "  #Data = Data[:10]\n",
    "  corpus_size = len(Data)\n",
    "  print \"The Total Corpus Data is about \" + str(corpus_size*2)\n",
    "  #cosine_similarity_without_tfidf_predicted_answers = cosine_similarity_without_tfidf(Data)\n",
    "  train_split = 0.8\n",
    "  training_data_documents_size = int(round(corpus_size * train_split))\n",
    "  #print training_data_documents_size\n",
    "  test_data_documents_size = corpus_size - training_data_documents_size\n",
    "  training_documents =  Data[:training_data_documents_size]\n",
    "  test_documents = Data[training_data_documents_size+1:]\n",
    "  print \"Training on \" + str(training_data_documents_size*2) + \" documents\"\n",
    "  print \"Testing on \" + str(test_data_documents_size) + \" documents\"\n",
    "  training_documents_answers = [item[2] for item in training_documents]\n",
    "  training_documents = [item[0:2] for item in training_documents]\n",
    "  training_documents = list(itertools.chain.from_iterable(training_documents))\n",
    "  test_documents_answers = [item[2] for item in test_documents]\n",
    "  test_documents = [item[0:2] for item in test_documents]\n",
    "  test_documents = list(itertools.chain.from_iterable(test_documents))\n",
    "  train_and_test_simple_model(training_documents, test_documents, training_documents_answers, test_documents_answers, load=True)\n",
    "\n",
    "\n",
    "def cosine_similarity_without_tfidf(documents):\n",
    "  answers = []\n",
    "  predicted_answers = []\n",
    "  ind=0\n",
    "  for document in documents:\n",
    "    answers.append(document[2])\n",
    "    predicted_answers.append(5 * ngram.cosinesimilarity_without_TFIDF(document[0], document[1]))\n",
    "    print answers[ind],   \"   |   \",  predicted_answers[ind]\n",
    "    ind+=1\n",
    "  print \"Error in Estimate is \" + str(evaluate(answers, predicted_answers))\n",
    "  return predicted_answers\n",
    "\n",
    "def evaluate(gold_standard, predicted_answers):\n",
    "  return np.sum(np.absolute(np.array(gold_standard) - np.array(predicted_answers)))/len(gold_standard) \n",
    "  # return math.sqrt(np.sum(np.power(np.array(gold_standard) - np.array(predicted_answers),2))) \n",
    "\n",
    "def train_and_test_simple_model(train_documents, test_documents, training_documents_answers, test_documents_answers, load=False):\n",
    "  if(not load):\n",
    "    TFIDFScores, Vocabulary, DocVectors, IDFVector = ngram.TFIDF(train_documents)\n",
    "    #print TFIDFScores\n",
    "    utility.save_weights(\"TFIDFScores.dat\",TFIDFScores);\n",
    "    utility.save_weights(\"Vocabulary.dat\", Vocabulary);\n",
    "    utility.save_weights(\"DocVectors.dat\", DocVectors);\n",
    "    utility.save_weights(\"IDFVector.dat\", IDFVector);\n",
    "  else:\n",
    "    TFIDFScores=utility.load_weights(\"weights/TFIDFScores.dat\")\n",
    "    #print TFIDFScores\n",
    "    Vocabulary=utility.load_weights(\"weights/Vocabulary.dat\")\n",
    "    DocVectors=utility.load_weights(\"weights/DocVectors.dat\")\n",
    "    IDFVector=utility.load_weights(\"weights/IDFVector.dat\")\n",
    "  f= open(\"analysis.txt\",\"w\")\n",
    "  print \"Training Documents Analysis\"\n",
    "  print \"-------------------------------------------------------------------\"\n",
    "  #train_predicted_answers = cosinesimilarity_evaluate_TFIDF(train_documents, TFIDFScores, training_documents_answers)\n",
    "  \n",
    "  print \"Test Documents Analysis\"\n",
    "  print \"-------------------------------------------------------------------\"\n",
    "  #test_predicted_answers = cosinesimilarity_evaluate_TFIDF(test_documents, TFIDFScores, test_documents_answers)\n",
    "  for ngram_size in range(1,2):\n",
    "    for analyze_type in [\"lemma\"]:#, \"character\"]:\n",
    "      if(analyze_type==\"character\"):\n",
    "        if(not load):\n",
    "          idf_scores = ngram.CharacterIDFVector(train_documents, ngram_size)\n",
    "          utility.save_weights(\"IDF_Char_\"+str(ngram_size)+\"_gram.dat\", idf_scores)\n",
    "        else:\n",
    "          idf_scores = utility.load_weights(\"weights/IDF_Char_\"+str(ngram_size)+\"_gram.dat\")\n",
    "      else:\n",
    "        idf_scores = IDFVector\n",
    "      # print \"-------------------------------------------------------------------\"\n",
    "      # f.write(\"-------------------------------------------------------------------\")\n",
    "      # print \"Jaccards and Containment Coefficient Analysis without using ngram weighing and type = \" + analyze_type + \" ngram = \" + str(ngram_size)\n",
    "      # f.write(\"Jaccards and Containment Coefficient Analysis without using ngram weighing and type = \" + analyze_type + \" ngram = \" + str(ngram_size))\n",
    "      # print \"-------------------------------------------------------------------\"\n",
    "      # f.write(\"-------------------------------------------------------------------\")\n",
    "      # Jacc_one_gram_pred_answers, Containment_one_gram_pred_answers = jaccard_and_containment_coefficient_evaluate(analyze_type, train_documents, training_documents_answers, ngram_size, False, IDFScores=None)\n",
    "      # Jacc_one_gram_pred_answers, Containment_one_gram_pred_answers = jaccard_and_containment_coefficient_evaluate(analyze_type, test_documents, test_documents_answers, ngram_size, False, IDFScores=None)\n",
    "      print \"-------------------------------------------------------------------\"\n",
    "      f.write(\"-------------------------------------------------------------------\")\n",
    "      print \"Jaccards and Containment Coefficient Analysis using ngram weighing and type = \" + analyze_type + \" ngram = \" + str(ngram_size)\n",
    "      f.write(\"Jaccards and Containment Coefficient Analysis using ngram weighing and type = \" + analyze_type + \" ngram = \" + str(ngram_size))\n",
    "      print \"-------------------------------------------------------------------\"\n",
    "      f.write(\"-------------------------------------------------------------------\")\n",
    "      #train_documents, training_documents_answers = [\"TEst this one\",\"Test this two\"],[3.0]\n",
    "      #test_documents, test_documents_answers = [\"TEst this one\",\"Test this two\"],[3.0]\n",
    "      Jacc_one_gram_pred_answers, Containment_one_gram_pred_answers = jaccard_and_containment_coefficient_evaluate(analyze_type, train_documents, training_documents_answers, ngram_size, True, idf_scores)\n",
    "      Jacc_one_gram_pred_answers, Containment_one_gram_pred_answers = jaccard_and_containment_coefficient_evaluate(analyze_type, test_documents, test_documents_answers, ngram_size, True, idf_scores)\n",
    "  f.close()\n",
    "\n",
    "def cosinesimilarity_evaluate_TFIDF(documents, TFIDFScores, answers):\n",
    "  ind = 0\n",
    "  predicted_answers = []\n",
    "  for i in range(len(documents)/2):\n",
    "    document1, document2 = documents[(2*i)], documents[(2*i)+1]\n",
    "    #print document1, document2\n",
    "    predicted_answers.append(5*ngram.cosinesimilarity(document1, document2, TFIDFScores))\n",
    "    #print answers[ind],   \"   |   \", predicted_answers[ind]\n",
    "    ind+=1\n",
    "  print \"Error in Estimate is \" + str(evaluate(answers, predicted_answers))\n",
    "  return predicted_answers \n",
    "\n",
    "def jaccard_and_containment_coefficient_evaluate(analyze_type, documents, answers, ngram_size=1, ngram_weighing=False, IDFScores=None):\n",
    "  ind = 0\n",
    "  containment_coefficient_predicted_answers = []\n",
    "  jaccard_coefficient_predicted_answers = []\n",
    "  for i in range(len(documents)/2):\n",
    "    document1, document2 = documents[(2*i)], documents[(2*i)+1]\n",
    "    #print document1, document2\n",
    "    sent_1_tokens = process.tokens(document1)\n",
    "    sent_2_tokens = process.tokens(document2)\n",
    "    if(analyze_type==\"pos\"):\n",
    "      jaccard_coefficient, containment_coefficient = ngram.POSTags_JaccardCoefficient_and_containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size, ngram_weighing, IDFScores)\n",
    "    elif(analyze_type==\"lemma\"):\n",
    "      jaccard_coefficient, containment_coefficient = ngram.Lemma_JaccardCoefficient_and_containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size, ngram_weighing, IDFScores)\n",
    "    elif(analyze_type==\"character\"):\n",
    "      jaccard_coefficient, containment_coefficient = ngram.character_ngram_JaccardCoefficient_and_containment_coefficienct(sent_1_tokens, sent_2_tokens, ngram_size, True, ngram_weighing, IDFScores)\n",
    "    jaccard_coefficient_predicted_answers.append(5*jaccard_coefficient)\n",
    "    containment_coefficient_predicted_answers.append(5*containment_coefficient)\n",
    "    #print answers[ind],   \"   |   \", jaccard_coefficient_predicted_answers[ind]\n",
    "    #print answers[ind],   \"   |   \", containment_coefficient_predicted_answers[ind]\n",
    "    ind+=1\n",
    "  print \"Error in Estimate For Jaccard Coefficient is \" + str(evaluate(answers, jaccard_coefficient_predicted_answers))\n",
    "  print \"Error in Estimate For Containment Coefficient is \" + str(evaluate(answers, containment_coefficient_predicted_answers))\n",
    "  return jaccard_coefficient_predicted_answers, containment_coefficient_predicted_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'test'] ['hi', 'test', 'this']\n",
      "['hi', 'test'] ['hi', 'test', 'this']\n",
      "set([('hi', 'test')]) set([('hi', 'test'), ('test', 'this')])\n",
      "hi\n",
      "test\n",
      "test\n",
      "this\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0794415416798357"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngram_weighted_value(keys, IDFScores, character_ngram=False):\n",
    "  value = 0.0\n",
    "  if not character_ngram:\n",
    "    for key in keys:\n",
    "      for i in range(len(key)):# key is a tuple\n",
    "        for j in range(len(key[i])):\n",
    "          print key[i][j]\n",
    "          if key[i][j] in IDFScores.keys():\n",
    "            value += IDFScores[key[i][j]]\n",
    "  else:\n",
    "    for key in keys:\n",
    "      if key in IDFScores.keys():\n",
    "        value+=IDFScores[key]\n",
    "  return value\n",
    "\n",
    "def ngram_weighted_value(keys, IDFScores, character_ngram=False):\n",
    "  value = 0.0\n",
    "  if not character_ngram:\n",
    "    for key in keys:\n",
    "      for i in range(len(key)):# This is a tuple\n",
    "        if(type(key[i]) is not tuple):\n",
    "          print key[i]\n",
    "          if key[i] in IDFScores.keys():\n",
    "            value += IDFScores[key[i]]\n",
    "        else:\n",
    "          print \"dan\"\n",
    "          print key[i][0]\n",
    "          if key[i][0] in IDFScores.keys():\n",
    "            value += IDFScores[key[i][0]] \n",
    "  else:\n",
    "    for key in keys:\n",
    "      if key in IDFScores.keys():\n",
    "        value+=IDFScores[key]\n",
    "  return value\n",
    "print tok1, tok2\n",
    "postag_tokens_1 = process.lemmatize(tok1)\n",
    "postag_tokens_2 = process.lemmatize(tok2)\n",
    "print postag_tokens_1,postag_tokens_2\n",
    "IDFScores = {'test': 0.6931471805599453, 'hi': 0.6931471805599453, 'i': 0.0, 'ike': 0.0}\n",
    "set1,set2= ngram_keys(postag_tokens_1,postag_tokens_2, 2,False)\n",
    "print set1, set2\n",
    "ngram_weighted_value(set2, IDFScores, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11022302463e-15\n",
      "[ 0.33333333  0.33333333  0.33333333]\n",
      "0.75\n",
      "[ 0.18181818 -0.07954545 -0.05681818]\n",
      "[ 1.23863636  0.5         0.75      ]\n",
      "Counter({'1': 3, '2': 2, '0': 1})\n"
     ]
    }
   ],
   "source": [
    "# create X and y\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper']\n",
    "X = [[1,1,1],[2,2,2],[3,3,3],]\n",
    "y = [1,2,3]\n",
    "\n",
    "# follow the usual sklearn pattern: import, instantiate, fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "\n",
    "# print intercept and coefficients\n",
    "print lm.intercept_\n",
    "print lm.coef_\n",
    "lm.predict([[4, 4, 4],[5, 5, 5]])\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=True)\n",
    "Corpus = [\"This is a test run\",\"Test this out i\"]\n",
    "D = [{'foo': 2, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "lm = LinearRegression()\n",
    "X = v.fit_transform(D)\n",
    "lm.fit(X, [1,0.5])\n",
    "# print intercept and coefficients\n",
    "print lm.intercept_\n",
    "print lm.coef_\n",
    "D2 = [{'foo': 1, 'bar': 3}, {'foo': 3, 'baz': 1},{'sss':1,'sdasda':1}]\n",
    "pX = v.transform(D2)\n",
    "print lm.predict(pX)\n",
    "\n",
    "print Counter(['1','2','1']) + Counter(['1','2','0'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
