{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielsampetethiyagu/anaconda/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import lib.collect_data as data_collection\n",
    "import models.ngram as ngram\n",
    "import models.linear_regression as lr\n",
    "import models.svm as svm\n",
    "import models.neuralnet_MLPRegressor as mlp\n",
    "import math as math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import lib.preprocess as process\n",
    "import lib.utilities as utility\n",
    "import pdb\n",
    "import models.w2vec as w2vec\n",
    "from semantic_similarity import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Corpus Data is about 29556\n",
      "Training on 23644 documents\n",
      "Testing on 5912 documents\n"
     ]
    }
   ],
   "source": [
    "Data = data_collection.Data();\n",
    "# Data = Data[:10]\n",
    "# Data = [[\"Its an orange\",\"Its orange\",3.5],[\"He is a bachelor\",\"He is an unmarried man\", 4.8]]\n",
    "corpus_size = len(Data)\n",
    "print \"The Total Corpus Data is about \" + str(corpus_size*2)\n",
    "train_split = 0.8\n",
    "training_data_documents_size = int(round(corpus_size * train_split))\n",
    "#print training_data_documents_size\n",
    "test_data_documents_size = corpus_size - training_data_documents_size\n",
    "training_documents =  Data[:training_data_documents_size]\n",
    "# print training_data_documents_size\n",
    "test_documents = Data[training_data_documents_size:]\n",
    "# print test_documents\n",
    "print \"Training on \" + str(training_data_documents_size*2) + \" documents\"\n",
    "print \"Testing on \" + str(test_data_documents_size*2) + \" documents\"\n",
    "training_documents_answers = [item[2] for item in training_documents]\n",
    "training_documents = [item[0:2] for item in training_documents]\n",
    "training_documents = list(itertools.chain.from_iterable(training_documents))\n",
    "test_documents_answers = [item[2] for item in test_documents]\n",
    "test_documents = [item[0:2] for item in test_documents]\n",
    "test_documents = list(itertools.chain.from_iterable(test_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING WORD2VEC MODEL\n",
      "LOADED WORD2VEC MODEL\n"
     ]
    }
   ],
   "source": [
    "w2vec_model = w2vec.w2vec_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{(('How', 'NNP'),): 2, ('you',): 4, ('are',): 4, (('Hi', 'NNP'),): 2, ('How', 'are'): 4, ('r', 'e'): 2, (('How', 'NNP'), ('are', 'VBP')): 2, ('are', 'you'): 4, (('Hi', 'NNP'), ('How', 'NNP')): 2, (('are', 'VBP'),): 2, ('Hi', 'How', 'are', 'you'): 4, (('are', 'VBP'), ('you', 'PRP')): 2, ('Hi', 'How', 'are'): 4, ('o', 'u'): 2, ('Hi',): 4, (('How', 'NNP'), ('are', 'VBP'), ('you', 'PRP')): 2, (('Hi', 'NNP'), ('How', 'NNP'), ('are', 'VBP'), ('you', 'PRP')): 2, ('How', 'are', 'you'): 4, ('H', 'o'): 2, (('Hi', 'NNP'), ('How', 'NNP'), ('are', 'VBP')): 2, ('y', 'o', 'u'): 4, ('H', 'o', 'w'): 4, (('you', 'PRP'),): 2, ('H', 'i'): 6, ('Hi', 'How'): 4, ('y', 'o'): 2, ('a', 'r', 'e'): 4, ('How',): 4, ('o', 'w'): 2, ('a', 'r'): 2}]\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d58c8c90bb4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hi How are you\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msentence_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hi How are you\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredict_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"svm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2vec_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_w2_vec_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/danielsampetethiyagu/github/semantic_similarity/semantic_similarity.py\u001b[0m in \u001b[0;36mpredict_sentences\u001b[0;34m(classifier_type, sentence_1, sentence_2, w2vec_model, use_w2_vec_model)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2vec_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_w2_vec_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"svm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2vec_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_w2_vec_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcosine_similarity_without_tfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielsampetethiyagu/github/semantic_similarity/models/svm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(sentence_1, sentence_2, w2vec_model, use_w2_vec_model)\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc_Dict_Vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mmin_max_scaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mX_train_minmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_max_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mpredicted_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_minmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielsampetethiyagu/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, copy)\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mscale\u001b[0m \u001b[0malong\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \"\"\"\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scale_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danielsampetethiyagu/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# FIXME NotFittedError_ --> NotFittedError in 0.19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_NotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "sentence_1 = \"Hi How are you\"\n",
    "sentence_2 = \"Hi How are you\"\n",
    "predict_sentences(\"svm\", sentence_1, sentence_2, w2vec_model, use_w2_vec_model=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
