{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppdbDict = {}\n",
    "ppdbSim = 0.9\n",
    "theta1 = 0.9\n",
    "punctuations = ['(','-lrb-','.',',','-','?','!',';','_',':','{','}','[','/',']','...','\"','\\'',')', '-rrb-']\n",
    "stemmer = SnowballStemmer('english')\n",
    "punctuations = ['(','-lrb-','.',',','-','?','!',';','_',':','{','}','[','/',']','...','\"','\\'',')', '-rrb-']\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadPPDB(ppdbFileName = 'monolingual-word-aligner/Resources/ppdb-1.0-xxxl-lexical.extended.synonyms.uniquepairs'):\n",
    "\n",
    "    global ppdbSim\n",
    "    global ppdbDict\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    ppdbFile = open(ppdbFileName, 'r')\n",
    "    for line in ppdbFile:\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        tokens[1] = tokens[1].strip()\n",
    "        ppdbDict[(tokens[0], tokens[1])] = ppdbSim\n",
    "        count += 1\n",
    "\n",
    "        \n",
    "def presentInPPDB(word1, word2):\n",
    "\n",
    "    global ppdbDict\n",
    "\n",
    "    if (word1.lower(), word2.lower()) in ppdbDict:\n",
    "        return True\n",
    "    if (word2.lower(), word1.lower()) in ppdbDict:\n",
    "        return True\n",
    "\n",
    "def wordRelatedness(word1, pos1, word2, pos2):\n",
    "\n",
    "    global stemmer\n",
    "    global ppdbSim\n",
    "    global punctuations\n",
    "\n",
    "    if len(word1) > 1:\n",
    "        canonicalWord1 = word1.replace('.', '')\n",
    "        canonicalWord1 = canonicalWord1.replace('-', '')\n",
    "        canonicalWord1 = canonicalWord1.replace(',', '')\n",
    "    else:\n",
    "        canonicalWord1 = word1\n",
    "        \n",
    "    if len(word2) > 1:\n",
    "        canonicalWord2 = word2.replace('.', '')\n",
    "        canonicalWord2 = canonicalWord2.replace('-', '')\n",
    "        canonicalWord2 = canonicalWord2.replace(',', '')\n",
    "    else:\n",
    "        canonicalWord2 = word2\n",
    "    \n",
    "    \n",
    "    if canonicalWord1.lower() == canonicalWord2.lower():\n",
    "        return 1\n",
    "\n",
    "    if stemmer.stem(word1).lower() == stemmer.stem(word2).lower():\n",
    "        return 1\n",
    "\n",
    "    if canonicalWord1.isdigit() and canonicalWord2.isdigit() and canonicalWord1 <> canonicalWord2:\n",
    "        return 0\n",
    "\n",
    "    if pos1.lower() == 'cd' and pos2.lower() == 'cd' and (not canonicalWord1.isdigit() and not canonicalWord2.isdigit()) and canonicalWord1 <> canonicalWord2:\n",
    "        return 0\n",
    "\n",
    "    # stopwords can be similar to only stopwords\n",
    "    if (word1.lower() in stopwords and word2.lower() not in stopwords) or (word1.lower() not in stopwords and word2.lower() in stopwords):\n",
    "        return 0\n",
    "\n",
    "    # punctuations can only be either identical or totally dissimilar\n",
    "    if word1 in punctuations or word2 in punctuations:\n",
    "        return 0\n",
    "\n",
    "    if presentInPPDB(word1.lower(), word2.lower()):\n",
    "        return ppdbSim\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loadPPDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"4\".isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findContinuousSublists(S1_tokens, S2_tokens, use_lower=True):\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    if(use_lower):\n",
    "        S1_tokens = map(lambda token: token.lower(), S1_tokens)\n",
    "        S2_tokens = map(lambda token: token.lower(), S2_tokens)\n",
    "    for token in S1_tokens:\n",
    "        s1.append(token)\n",
    "    for token in S2_tokens:\n",
    "        s2.append(token)\n",
    "    \n",
    "    s1_size = len(s1)\n",
    "    commonsublists = []\n",
    "    for size in xrange(s1_size,0,-1):\n",
    "        starting_indices_S1 = [item for item in xrange(0,len(s1)-size+1)]\n",
    "        starting_indices_S2 = [item for item in xrange(0,len(s2)-size+1)]\n",
    "        for i in starting_indices_S1:\n",
    "            for j in starting_indices_S2:\n",
    "                if(s1[i:i+size]==s2[j:j+size]):\n",
    "                    alreadyInserted = False\n",
    "                    currentS1_indices = [ind for ind in xrange(i,i+size)]\n",
    "                    currentS2_indices = [ind for ind in xrange(j,j+size)]\n",
    "                    for item in commonsublists:\n",
    "                        if(isSublist(currentS1_indices,item[0]) and isSublist(currentS2_indices, item[1])):\n",
    "                            alreadyInserted = True\n",
    "                            break;\n",
    "                    if not alreadyInserted:\n",
    "                        commonsublists.append([currentS1_indices, currentS2_indices])\n",
    "    return commonsublists\n",
    "\n",
    "def isSublist(A, B):\n",
    "# returns True if A is a sublist of B, False otherwise\n",
    "    sub = True\n",
    "\n",
    "    for item in A:\n",
    "        if item not in B:\n",
    "            sub = False\n",
    "            break\n",
    "    \n",
    "    return sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import coreNlpUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from coreNlpUtil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findAllCommonContiguousSublists(A, B, turnToLowerCases=True): # this is a very inefficient implementation, you can use suffix trees to devise a much faster method\n",
    "# returns all the contiguous sublists in order of decreasing length\n",
    "# output format (0-indexed):\n",
    "# [\n",
    "#    [[indices in 'A' for common sublist 1], [indices in 'B' for common sublist 1]],\n",
    "#    ...,\n",
    "#    [[indices in 'A' for common sublist n], [indices in 'B' for common sublist n]]\n",
    "# ]\n",
    "\n",
    "    a = []\n",
    "    b = []\n",
    "    for item in A:\n",
    "        a.append(item)\n",
    "    for item in B:\n",
    "        b.append(item)\n",
    "\n",
    "    if turnToLowerCases:\n",
    "        for i in xrange(len(a)):\n",
    "            a[i] = a[i].lower()\n",
    "        for i in xrange(len(b)):\n",
    "            b[i] = b[i].lower()\n",
    "            \n",
    "\n",
    "    commonContiguousSublists = []\n",
    "\n",
    "    swapped = False\n",
    "    if len(a) > len(b):\n",
    "        temp = a\n",
    "        a = b\n",
    "        b = temp\n",
    "        swapped = True\n",
    "\n",
    "    maxSize = len(a)\n",
    "    for size in xrange(maxSize, 0, -1):\n",
    "        startingIndicesForA = [item for item in xrange(0, len(a)-size+1)]\n",
    "        startingIndicesForB = [item for item in xrange(0, len(b)-size+1)]\n",
    "        for i in startingIndicesForA:\n",
    "            for j in startingIndicesForB:\n",
    "                if a[i:i+size] == b[j:j+size]:\n",
    "                    # check if a contiguous superset has already been inserted; don't insert this one in that case\n",
    "                    alreadyInserted = False\n",
    "                    currentAIndices = [item for item in xrange(i,i+size)]\n",
    "                    currentBIndices = [item for item in xrange(j,j+size)]\n",
    "                    for item in commonContiguousSublists:\n",
    "                        if isSublist(currentAIndices, item[0]) and isSublist(currentBIndices, item[1]):\n",
    "                            alreadyInserted = True\n",
    "                            break\n",
    "                    if not alreadyInserted:\n",
    "                        commonContiguousSublists.append([currentAIndices, currentBIndices])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    if swapped:\n",
    "        for item in commonContiguousSublists:\n",
    "            temp = item[0]\n",
    "            item[0] = item[1]\n",
    "            item[1] = temp\n",
    "\n",
    "                        \n",
    "    return commonContiguousSublists\n",
    "\n",
    "def isSublist(A, B):\n",
    "# returns True if A is a sublist of B, False otherwise\n",
    "    sub = True\n",
    "\n",
    "    for item in A:\n",
    "        if item not in B:\n",
    "            sub = False\n",
    "            break\n",
    "    \n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = [\"check\",\"this\",\"Coke\",\"out\",\"isnt\",\"this\",\"wonderful\"]\n",
    "s2 = [\"test\",\"this\",\"pepsi\",\"out\",\"dude\",\"isn't\",\"this\",\"wonderful\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 2], [1, 2]], [[4, 5], [5, 6]], [[1], [5]], [[4], [1]]]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findAllCommonContiguousSublists([\"check\",\"this\",\"out\",\"isnt\",\"this\",\"wonderful\"], [\"test\",\"this\",\"out\",\"dude\",\"isn't\",\"this\",\"wonderful\"], turnToLowerCases=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence1 = \"check this Daniel Sam out isnt this wonderful Indian Ocean\"\n",
    "sentence2 = \"test this Dany out dude isn't this wonderful India\"\n",
    "if isinstance(sentence1, list):\n",
    "    sentence1 = ' '.join(sentence1)\n",
    "if isinstance(sentence2, list):\n",
    "    sentence2 = ' '.joinsentence1ParseResult(sentence2)\n",
    "\n",
    "sentence1ParseResult = parseText(sentence1)\n",
    "sentence2ParseResult = parseText(sentence2)\n",
    "\n",
    "sentence1Lemmatized = lemmatize(sentence1ParseResult)\n",
    "sentence2Lemmatized = lemmatize(sentence2ParseResult)\n",
    "\n",
    "sentence1PosTagged = posTag(sentence1ParseResult)\n",
    "sentence2PosTagged = posTag(sentence2ParseResult)\n",
    "sentence1LemmasAndPosTags = []\n",
    "\n",
    "for i in xrange(len(sentence1Lemmatized)):\n",
    "    sentence1LemmasAndPosTags.append([])    \n",
    "for i in xrange(len(sentence1Lemmatized)):\n",
    "    for item in sentence1Lemmatized[i]:\n",
    "        sentence1LemmasAndPosTags[i].append(item)\n",
    "    sentence1LemmasAndPosTags[i].append(sentence1PosTagged[i][3])\n",
    "\n",
    "sentence2LemmasAndPosTags = []\n",
    "for i in xrange(len(sentence2Lemmatized)):\n",
    "    sentence2LemmasAndPosTags.append([])    \n",
    "for i in xrange(len(sentence2Lemmatized)):\n",
    "    for item in sentence2Lemmatized[i]:\n",
    "        sentence2LemmasAndPosTags[i].append(item)\n",
    "    sentence2LemmasAndPosTags[i].append(sentence2PosTagged[i][3])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[6, 7], [7, 8]], [[1], [1]], [[1], [7]], [[4], [3]], [[6], [1]]]\n"
     ]
    }
   ],
   "source": [
    "source = sentence1LemmasAndPosTags \n",
    "target = sentence2LemmasAndPosTags\n",
    "sourceWordIndices = [i+1 for i in xrange(len(source))]\n",
    "targetWordIndices = [i+1 for i in xrange(len(target))]\n",
    "\n",
    "\n",
    "alignments = []\n",
    "sourceWordIndicesAlreadyAligned= []\n",
    "targetWordIndicesAlreadyAligned= []\n",
    "\n",
    "sourceWords = [item[2] for item in source]\n",
    "targetWords = [item[2] for item in target]\n",
    "\n",
    "sourceLemmas = [item[3] for item in source]\n",
    "targetLemmas = [item[3] for item in target]\n",
    "\n",
    "sourcePosTags = [item[4] for item in source]\n",
    "targetPosTags = [item[4] for item in target]\n",
    "commonContiguousSublists  = findAllCommonContiguousSublists(sourceWords, targetWords, True)\n",
    "print commonContiguousSublists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "commonContiguousSublists  = findContinuousSublists(sourceWords, targetWords, True)\n",
    "for item in commonContiguousSublists:\n",
    "    if len(item[0]) >= 2:\n",
    "        for j in xrange(len(item[0])):\n",
    "            if item[0][j]+1 not in sourceWordIndicesAlreadyAligned and item[1][j]+1 not in targetWordIndicesAlreadyAligned and [item[0][j]+1, item[1][j]+1] not in alignments:\n",
    "                alignments.append([item[0][j]+1, item[1][j]+1])\n",
    "                sourceWordIndicesAlreadyAligned.append(item[0][j]+1)\n",
    "                targetWordIndicesAlreadyAligned.append(item[1][j]+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sourceNamedEntities = ner(sentence1ParseResult)\n",
    "sourceNamedEntities = sorted(sourceNamedEntities, key=len)\n",
    "targetNamedEntities = ner(sentence2ParseResult)\n",
    "targetNamedEntities = sorted(targetNamedEntities, key=len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 8], [8, 9]]\n"
     ]
    }
   ],
   "source": [
    "print alignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def align_Named_Entities(source, target, sourceParseResult, targetParseResult, existingAlignments):\n",
    "    \n",
    "    global punctuations\n",
    "\n",
    "    alignments = []\n",
    "\n",
    "    sourceNamedEntities = ner(sourceParseResult)\n",
    "    sourceNamedEntities = sorted(sourceNamedEntities, key=len)\n",
    "    \n",
    "    targetNamedEntities = ner(targetParseResult)\n",
    "    targetNamedEntities = sorted(targetNamedEntities, key=len)\n",
    "    \n",
    "\n",
    "    sourceNamedEntitiesAlreadyAligned = []\n",
    "    targetNamedEntitiesAlreadyAligned = []\n",
    "\n",
    "\n",
    "    # align all full matches\n",
    "    for item in sourceNamedEntities:\n",
    "        if item[3] not in ['PERSON', 'ORGANIZATION', 'LOCATION']:\n",
    "            continue\n",
    "\n",
    "        for jtem in targetNamedEntities:\n",
    "            print jtem\n",
    "            if jtem[3] not in ['PERSON', 'ORGANIZATION', 'LOCATION']:\n",
    "                continue\n",
    "\n",
    "            # get rid of dots and hyphens\n",
    "            canonicalItemWord = [i.replace('.', '') for i in item[2]]\n",
    "            canonicalItemWord = [i.replace('-', '') for i in item[2]]\n",
    "            canonicalJtemWord = [j.replace('.', '') for j in jtem[2]]\n",
    "            canonicalJtemWord = [j.replace('-', '') for j in jtem[2]]\n",
    "\n",
    "            if canonicalItemWord == canonicalJtemWord:\n",
    "                for k in xrange(len(item[1])):\n",
    "                    print k\n",
    "                    if ([item[1][k], jtem[1][k]]) not in alignments:\n",
    "                        alignments.append([item[1][k], jtem[1][k]])\n",
    "                sourceNamedEntitiesAlreadyAligned.append(item)\n",
    "                targetNamedEntitiesAlreadyAligned.append(jtem)\n",
    "    \n",
    "    \n",
    "    # align subset matches\n",
    "    for item in sourceNamedEntities:\n",
    "        if item[3] not in ['PERSON', 'ORGANIZATION', 'LOCATION'] or item in sourceNamedEntitiesAlreadyAligned:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        for jtem in targetNamedEntities:\n",
    "            \n",
    "            # find if the first is a part of the second\n",
    "            print item, jtem\n",
    "            print item[2], jtem[2]\n",
    "            if isSublist(item[2], jtem[2]):\n",
    "                unalignedWordIndicesInTheLongerName = []\n",
    "                for ktem in jtem[1]:\n",
    "                    unalignedWordIndicesInTheLongerName.append(ktem)\n",
    "                for k in xrange(len(item[2])):\n",
    "                    for l in xrange(len(jtem[2])):\n",
    "                        if item[2][k] == jtem[2][l] and [item[1][k], jtem[1][l]] not in alignments:\n",
    "                            alignments.append([item[1][k], jtem[1][l]])\n",
    "                            if jtem[1][l] in unalignedWordIndicesInTheLongerName:\n",
    "                                unalignedWordIndicesInTheLongerName.remove(jtem[1][l])\n",
    "                for k in xrange(len(item[1])): # the shorter name\n",
    "                    for l in xrange(len(jtem[1])): # the longer name\n",
    "                        # find if the current term in the longer name has already been aligned (before calling alignNamedEntities()), do not align it in that case\n",
    "                        alreadyInserted = False\n",
    "                        for mtem in existingAlignments:\n",
    "                            if mtem[1] == jtem[1][l]:\n",
    "                                alreadyInserted = True\n",
    "                                break\n",
    "                        if jtem[1][l] not in unalignedWordIndicesInTheLongerName or alreadyInserted:\n",
    "                            continue\n",
    "                        if [item[1][k], jtem[1][l]] not in alignments  and target[jtem[1][l]-1][2] not in sourceWords  and item[2][k] not in punctuations and jtem[2][l] not in punctuations:\n",
    "                            alignments.append([item[1][k], jtem[1][l]])\n",
    "                \n",
    "            # else find if the second is a part of the first\n",
    "            elif isSublist(jtem[2], item[2]):\n",
    "                unalignedWordIndicesInTheLongerName = []\n",
    "                for ktem in item[1]:\n",
    "                    unalignedWordIndicesInTheLongerName.append(ktem)\n",
    "                for k in xrange(len(jtem[2])):\n",
    "                    for l in xrange(len(item[2])):\n",
    "                        if jtem[2][k] == item[2][l] and [item[1][l], jtem[1][k]] not in alignments:\n",
    "                            alignments.append([item[1][l], jtem[1][k]])\n",
    "                            if item[1][l] in unalignedWordIndicesInTheLongerName:\n",
    "                                unalignedWordIndicesInTheLongerName.remove(item[1][l])\n",
    "                for k in xrange(len(jtem[1])): # the shorter name\n",
    "                    for l in xrange(len(item[1])): # the longer name\n",
    "                        # find if the current term in the longer name has already been aligned (before calling alignNamedEntities()), do not align it in that case\n",
    "                        alreadyInserted = False\n",
    "                        for mtem in existingAlignments:\n",
    "                            print mtem\n",
    "                            if mtem[0] == item[1][k]:\n",
    "                                alreadyInserted = True\n",
    "                                break\n",
    "                        if item[1][l] not in unalignedWordIndicesInTheLongerName or alreadyInserted:\n",
    "                            continue\n",
    "                        if [item[1][l], jtem[1][k]] not in alignments  and source[item[1][k]-1][2] not in targetWords  and item[2][l] not in punctuations and jtem[2][k] not in punctuations:\n",
    "                            alignments.append([item[1][l], jtem[1][k]])\n",
    "                            #unalignedWordIndicesInTheLongerName.remove(jtem[1][l])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[u'10', u'14']], [3], [u'Dany'], u'PERSON']\n",
      "[[[u'45', u'50']], [10], [u'India'], u'LOCATION']\n",
      "[[[u'10', u'14']], [3], [u'Dany'], u'PERSON']\n",
      "[[[u'45', u'50']], [10], [u'India'], u'LOCATION']\n",
      "[[[u'11', u'17'], [u'18', u'21']], [3, 4], [u'Daniel', u'Sam'], u'PERSON'] [[[u'10', u'14']], [3], [u'Dany'], u'PERSON']\n",
      "[u'Daniel', u'Sam'] [u'Dany']\n",
      "[[[u'11', u'17'], [u'18', u'21']], [3, 4], [u'Daniel', u'Sam'], u'PERSON'] [[[u'45', u'50']], [10], [u'India'], u'LOCATION']\n",
      "[u'Daniel', u'Sam'] [u'India']\n",
      "[[[u'46', u'52'], [u'53', u'58']], [9, 10], [u'Indian', u'Ocean'], u'LOCATION'] [[[u'10', u'14']], [3], [u'Dany'], u'PERSON']\n",
      "[u'Indian', u'Ocean'] [u'Dany']\n",
      "[[[u'46', u'52'], [u'53', u'58']], [9, 10], [u'Indian', u'Ocean'], u'LOCATION'] [[[u'45', u'50']], [10], [u'India'], u'LOCATION']\n",
      "[u'Indian', u'Ocean'] [u'India']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print align_Named_Entities(sentence1LemmasAndPosTags, sentence2LemmasAndPosTags, sentence1ParseResult, sentence2ParseResult, alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alignNamedEntities(source, target, sourceParseResult, targetParseResult, existingAlignments):\n",
    "# source and target:: each is a list of elements of the form:\n",
    "# [[character begin offset, character end offset], word index, word, lemma, pos tag]\n",
    "\n",
    "    global punctuations\n",
    "\n",
    "    alignments = []\n",
    "\n",
    "    sourceNamedEntities = ner(sourceParseResult)\n",
    "    sourceNamedEntities = sorted(sourceNamedEntities, key=len)\n",
    "    \n",
    "    targetNamedEntities = ner(targetParseResult)\n",
    "    targetNamedEntities = sorted(targetNamedEntities, key=len)\n",
    "    \n",
    "\n",
    "    # learn from the other sentence that a certain word/phrase is a named entity (learn for source from target)\n",
    "    for item in source:\n",
    "        alreadyIncluded = False\n",
    "        for jtem in sourceNamedEntities:\n",
    "            if item[1] in jtem[1]:\n",
    "                alreadyIncluded = True\n",
    "                break\n",
    "        if alreadyIncluded or (len(item[2]) >0 and not item[2][0].isupper()):\n",
    "            continue\n",
    "        for jtem in targetNamedEntities:\n",
    "            if item[2] in jtem[2]:\n",
    "                # construct the item\n",
    "                newItem = [[item[0]], [item[1]], [item[2]], jtem[3]]\n",
    "\n",
    "                # check if the current item is part of a named entity part of which has already been added (by checking contiguousness)\n",
    "                partOfABiggerName = False\n",
    "                for k in xrange(len(sourceNamedEntities)):\n",
    "                    if sourceNamedEntities[k][1][len(sourceNamedEntities[k][1])-1] == newItem[1][0] - 1:\n",
    "                        sourceNamedEntities[k][0].append(newItem[0][0])\n",
    "                        sourceNamedEntities[k][1].append(newItem[1][0])\n",
    "                        sourceNamedEntities[k][2].append(newItem[2][0])\n",
    "                        partOfABiggerName = True\n",
    "                if not partOfABiggerName:\n",
    "                    sourceNamedEntities.append(newItem)\n",
    "            elif isAcronym(item[2], jtem[2]) and [[item[0]], [item[1]], [item[2]], jtem[3]] not in sourceNamedEntities:\n",
    "                sourceNamedEntities.append([[item[0]], [item[1]], [item[2]], jtem[3]])\n",
    "\n",
    "\n",
    "    \n",
    "    # learn from the other sentence that a certain word/phrase is a named entity (learn for target from source)\n",
    "    for item in target:\n",
    "        alreadyIncluded = False\n",
    "        for jtem in targetNamedEntities:\n",
    "            if item[1] in jtem[1]:\n",
    "                alreadyIncluded = True\n",
    "                break\n",
    "        if alreadyIncluded or (len(item[2]) >0 and not item[2][0].isupper()):\n",
    "            continue\n",
    "        for jtem in sourceNamedEntities:\n",
    "            if item[2] in jtem[2]:\n",
    "                # construct the item\n",
    "                newItem = [[item[0]], [item[1]], [item[2]], jtem[3]]\n",
    "\n",
    "                # check if the current item is part of a named entity part of which has already been added (by checking contiguousness)\n",
    "                partOfABiggerName = False\n",
    "                for k in xrange(len(targetNamedEntities)):\n",
    "                    if targetNamedEntities[k][1][len(targetNamedEntities[k][1])-1] == newItem[1][0] - 1:\n",
    "                        targetNamedEntities[k][0].append(newItem[0][0])\n",
    "                        targetNamedEntities[k][1].append(newItem[1][0])\n",
    "                        targetNamedEntities[k][2].append(newItem[2][0])\n",
    "                        partOfABiggerName = True\n",
    "                if not partOfABiggerName:\n",
    "                    targetNamedEntities.append(newItem)\n",
    "            elif isAcronym(item[2], jtem[2]) and [[item[0]], [item[1]], [item[2]], jtem[3]] not in targetNamedEntities:\n",
    "                targetNamedEntities.append([[item[0]], [item[1]], [item[2]], jtem[3]])\n",
    "    \n",
    "\n",
    "    sourceWords = []\n",
    "    targetWords = []\n",
    "\n",
    "    for item in sourceNamedEntities:\n",
    "        for jtem in item[1]:\n",
    "            if item[3] in ['PERSON', 'ORGANIZATION', 'LOCATION']:\n",
    "                sourceWords.append(source[jtem-1][2])\n",
    "    for item in targetNamedEntities:\n",
    "        for jtem in item[1]:\n",
    "            if item[3] in ['PERSON', 'ORGANIZATION', 'LOCATION']:\n",
    "                targetWords.append(target[jtem-1][2])\n",
    "\n",
    "\n",
    "\n",
    "    if len(sourceNamedEntities) == 0 or len(targetNamedEntities) == 0:\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sourceNamedEntitiesAlreadyAligned = []\n",
    "    targetNamedEntitiesAlreadyAligned = []\n",
    "    \n",
    "\n",
    "    # align all full matches\n",
    "    for item in sourceNamedEntities:\n",
    "        if item[3] not in ['PERSON', 'ORGANIZATION', 'LOCATION']:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # do not align if the current source entity is present more than once\n",
    "        count = 0\n",
    "        for ktem in sourceNamedEntities:\n",
    "            if ktem[2] == item[2]:\n",
    "                count += 1\n",
    "        if count > 1:\n",
    "            continue\n",
    "\n",
    "\n",
    "        for jtem in targetNamedEntities:\n",
    "            if jtem[3] not in ['PERSON', 'ORGANIZATION', 'LOCATION']:\n",
    "                continue\n",
    "\n",
    "            # do not align if the current target entity is present more than once\n",
    "            count = 0\n",
    "            for ktem in targetNamedEntities:\n",
    "                if ktem[2] == jtem[2]:\n",
    "                    count += 1\n",
    "            if count > 1:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # get rid of dots and hyphens\n",
    "            canonicalItemWord = [i.replace('.', '') for i in item[2]]\n",
    "            canonicalItemWord = [i.replace('-', '') for i in item[2]]\n",
    "            canonicalJtemWord = [j.replace('.', '') for j in jtem[2]]\n",
    "            canonicalJtemWord = [j.replace('-', '') for j in jtem[2]]\n",
    "\n",
    "            if canonicalItemWord == canonicalJtemWord:\n",
    "                for k in xrange(len(item[1])):\n",
    "                    if ([item[1][k], jtem[1][k]]) not in alignments:\n",
    "                        alignments.append([item[1][k], jtem[1][k]])\n",
    "                sourceNamedEntitiesAlreadyAligned.append(item)\n",
    "                targetNamedEntitiesAlreadyAligned.append(jtem)\n",
    "\n",
    "    # align acronyms with their elaborations\n",
    "    for item in sourceNamedEntities:\n",
    "        if item[3] not in ['PERSON', 'ORGANIZATION', 'LOCATION']:\n",
    "            continue\n",
    "        for jtem in targetNamedEntities:\n",
    "            if jtem[3] not in ['PERSON', 'ORGANIZATION', 'LOCATION']:\n",
    "                continue\n",
    "\n",
    "            if len(item[2])==1 and isAcronym(item[2][0], jtem[2]):\n",
    "                for i in xrange(len(jtem[1])):\n",
    "                    if [item[1][0], jtem[1][i]] not in alignments:\n",
    "                        alignments.append([item[1][0], jtem[1][i]])\n",
    "                        sourceNamedEntitiesAlreadyAligned.append(item[1][0])\n",
    "                        targetNamedEntitiesAlreadyAligned.append(jtem[1][i])\n",
    "\n",
    "            elif len(jtem[2])==1 and isAcronym(jtem[2][0], item[2]):\n",
    "                for i in xrange(len(item[1])):\n",
    "                    if [item[1][i], jtem[1][0]] not in alignments:\n",
    "                        alignments.append([item[1][i], jtem[1][0]])\n",
    "                        sourceNamedEntitiesAlreadyAligned.append(item[1][i])\n",
    "                        targetNamedEntitiesAlreadyAligned.append(jtem[1][0])\n",
    "\n",
    "                               \n",
    "    # align subset matches\n",
    "    for item in sourceNamedEntities:\n",
    "        if item[3] not in ['PERSON', 'ORGANIZATION', 'LOCATION'] or item in sourceNamedEntitiesAlreadyAligned:\n",
    "            continue\n",
    "\n",
    "        # do not align if the current source entity is present more than once\n",
    "        count = 0\n",
    "        for ktem in sourceNamedEntities:\n",
    "            if ktem[2] == item[2]:\n",
    "                count += 1\n",
    "        if count > 1:\n",
    "            continue\n",
    "\n",
    "\n",
    "        for jtem in targetNamedEntities:\n",
    "            if jtem[3] not in ['PERSON', 'ORGANIZATION', 'LOCATION'] or jtem in targetNamedEntitiesAlreadyAligned:\n",
    "                continue\n",
    "\n",
    "            if item[3] <> jtem[3]:\n",
    "                continue\n",
    "\n",
    "            # do not align if the current target entity is present more than once\n",
    "            count = 0\n",
    "            for ktem in targetNamedEntities:\n",
    "                if ktem[2] == jtem[2]:\n",
    "                    count += 1\n",
    "            if count > 1:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            # find if the first is a part of the second\n",
    "            if isSublist(item[2], jtem[2]):\n",
    "                unalignedWordIndicesInTheLongerName = []\n",
    "                for ktem in jtem[1]:\n",
    "                    unalignedWordIndicesInTheLongerName.append(ktem)\n",
    "                for k in xrange(len(item[2])):\n",
    "                    for l in xrange(len(jtem[2])):\n",
    "                        if item[2][k] == jtem[2][l] and [item[1][k], jtem[1][l]] not in alignments:\n",
    "                            alignments.append([item[1][k], jtem[1][l]])\n",
    "                            if jtem[1][l] in unalignedWordIndicesInTheLongerName:\n",
    "                                unalignedWordIndicesInTheLongerName.remove(jtem[1][l])\n",
    "                for k in xrange(len(item[1])): # the shorter name\n",
    "                    for l in xrange(len(jtem[1])): # the longer name\n",
    "                        # find if the current term in the longer name has already been aligned (before calling alignNamedEntities()), do not align it in that case\n",
    "                        alreadyInserted = False\n",
    "                        for mtem in existingAlignments:\n",
    "                            if mtem[1] == jtem[1][l]:\n",
    "                                alreadyInserted = True\n",
    "                                break\n",
    "                        if jtem[1][l] not in unalignedWordIndicesInTheLongerName or alreadyInserted:\n",
    "                            continue\n",
    "                        if [item[1][k], jtem[1][l]] not in alignments  and target[jtem[1][l]-1][2] not in sourceWords  and item[2][k] not in punctuations and jtem[2][l] not in punctuations:\n",
    "                            alignments.append([item[1][k], jtem[1][l]])\n",
    "                \n",
    "            # else find if the second is a part of the first\n",
    "            elif isSublist(jtem[2], item[2]):\n",
    "                unalignedWordIndicesInTheLongerName = []\n",
    "                for ktem in item[1]:\n",
    "                    unalignedWordIndicesInTheLongerName.append(ktem)\n",
    "                for k in xrange(len(jtem[2])):\n",
    "                    for l in xrange(len(item[2])):\n",
    "                        if jtem[2][k] == item[2][l] and [item[1][l], jtem[1][k]] not in alignments:\n",
    "                            alignments.append([item[1][l], jtem[1][k]])\n",
    "                            if item[1][l] in unalignedWordIndicesInTheLongerName:\n",
    "                                unalignedWordIndicesInTheLongerName.remove(item[1][l])\n",
    "                for k in xrange(len(jtem[1])): # the shorter name\n",
    "                    for l in xrange(len(item[1])): # the longer name\n",
    "                        # find if the current term in the longer name has already been aligned (before calling alignNamedEntities()), do not align it in that case\n",
    "                        alreadyInserted = False\n",
    "                        for mtem in existingAlignments:\n",
    "                            if mtem[0] == item[1][k]:\n",
    "                                alreadyInserted = True\n",
    "                                break\n",
    "                        if item[1][l] not in unalignedWordIndicesInTheLongerName or alreadyInserted:\n",
    "                            continue\n",
    "                        if [item[1][l], jtem[1][k]] not in alignments  and source[item[1][k]-1][2] not in targetWords  and item[2][l] not in punctuations and jtem[2][k] not in punctuations:\n",
    "                            alignments.append([item[1][l], jtem[1][k]])\n",
    "                            #unalignedWordIndicesInTheLongerName.remove(jtem[1][l])\n",
    "\n",
    "                \n",
    "\n",
    "    return alignments\n",
    "\n",
    "def isAcronym(word, namedEntity):\n",
    "# returns whether 'word' is an acronym of 'namedEntity', which is a list of the component words\n",
    "    canonicalWord = word.replace('.', '')\n",
    "    if not canonicalWord.isupper() or len(canonicalWord) <> len(namedEntity) or canonicalWord.lower() in ['a', 'i']:\n",
    "        return False\n",
    "\n",
    "    acronym = True    \n",
    "    for i in xrange(len(canonicalWord)):\n",
    "        if canonicalWord[i] <> namedEntity[i][0]:\n",
    "            acronym = False\n",
    "            break\n",
    "\n",
    "    return acronym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 8], [8, 9]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print alignments\n",
    "print alignNamedEntities(sentence1LemmasAndPosTags, sentence2LemmasAndPosTags, sentence1ParseResult, sentence2ParseResult, alignments)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
